{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NER model evaluation notebook\n",
        "Author name: Daniel J. S. Bright  \n",
        "Author contact: 12004727@uhi.ac.uk  \n",
        "Date last touched: 23 February 2023  \n",
        "Description: Jupyter Notebook sheet to test the performance of NER systems. Presents speed of NER execution and calculations for Precision, Recall and F1-Score. Created for a dissertation for the MSc in Web Technologies at University of the Highlands & Islands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7EXnt9uUSv5"
      },
      "outputs": [],
      "source": [
        "# install the required Python libraries\n",
        "!pipenv install spacy numpy pandas spacy_stanza spacy-transformers   # install for pipenv environment\n",
        "#!pip install spacy numpy pandas spacy_stanza spacy-transformers   # install for Google Colab or other regular pip environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uesVopp8ZsoB"
      },
      "outputs": [],
      "source": [
        "# download the spaCy models\n",
        "!python -m spacy download en_core_web_lg \n",
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ2KMXaXVBEM"
      },
      "outputs": [],
      "source": [
        "# function to mount Google Drive storage\n",
        "def mount_google_drive():\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPQc5ddUUSv6"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import spacy, json, glob, os, stanza, spacy_stanza, spacy_transformers\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "\n",
        "# function to get the URLs\n",
        "def get_annotation_urls(url, ext, print_output=True):\n",
        "    global annotated_files\n",
        "    # ensure url has trailing slash\n",
        "    url = url + \"/\" if url[-1:] != \"/\" else url\n",
        "    # load hand annotated examples\n",
        "    annotated_files = glob.glob(url + f\"*.{ext}\")\n",
        "    # sort based on filename\n",
        "    annotated_files.sort(key=lambda x: os.path.basename(x))\n",
        "    # print counted files to demonstrate success\n",
        "    if print_output:\n",
        "        print(f\"Number of annotated files: {len(annotated_files)}\")\n",
        "\n",
        "\n",
        "# function to put JSON data into Python dictionaries\n",
        "def json_to_doc(print_output=False):\n",
        "    # Load json into list of Python dictionaries\n",
        "    global annotations\n",
        "    annotations = []\n",
        "    for f in annotated_files:\n",
        "        with open(f, \"r\", encoding=\"utf-8\") as file:\n",
        "            annotations.append(json.loads(file.read()))\n",
        "    if print_output:\n",
        "        # print count of annotation dictionaries to verify success\n",
        "        print(f\"Number of annotations in files: {len(annotations)}\")\n",
        "        # print first element (document), to verify\n",
        "        print(f\"Annotation sample: {annotations[:1]}\")\n",
        "\n",
        "\n",
        "# function to load the models\n",
        "def load_models(print_output, model):\n",
        "    global nlp, nlp_trf_orig\n",
        "    if model == \"stanza\":\n",
        "        stanza.download(\"en\")\n",
        "        nlp = spacy_stanza.load_pipeline(\"en\")\n",
        "    elif model in [\"trf-model-best\", \"trf-model-best-tuned\", \"cnn-model-best\"]:\n",
        "        nlp = spacy.load(\n",
        "            trained_model_path + model,\n",
        "            exclude=\"parser,tagger,attribute_ruler,lemmatizer\",\n",
        "        )\n",
        "        nlp_trf_orig = spacy.load(\n",
        "            \"en_core_web_trf\"\n",
        "            if model in [\"trf-model-best\", \"trf-model-best-tuned\"]\n",
        "            else \"en_core_web_lg\"\n",
        "        )  # required as workaround to frozen components bug\n",
        "        nlp.add_pipe(\n",
        "            \"parser\",\n",
        "            source=nlp_trf_orig,\n",
        "            after=\"transformer\"\n",
        "            if model in [\"trf-model-best\", \"trf-model-best-tuned\"]\n",
        "            else \"tok2vec\",\n",
        "        )\n",
        "        nlp.add_pipe(\"tagger\", source=nlp_trf_orig, after=\"parser\")\n",
        "        nlp.add_pipe(\"attribute_ruler\", source=nlp_trf_orig, after=\"tagger\")\n",
        "        nlp.add_pipe(\"lemmatizer\", source=nlp_trf_orig, after=\"attribute_ruler\")\n",
        "        print(f\"Evaluating model: {model}\") if print_output else None\n",
        "    else:\n",
        "        model = (\n",
        "            trained_model_path + model\n",
        "            if model not in [\"en_core_web_lg\", \"en_core_web_trf\"]\n",
        "            else model\n",
        "        )\n",
        "        print(f\"Evaluating model: {model}\") if print_output else None\n",
        "        nlp = spacy.load(model)\n",
        "\n",
        "\n",
        "# function to set up the paths, etc.\n",
        "def setup(model, dataset, print_output=False, colab=0):\n",
        "    global trained_model_path, doc_results_path, doc_results_filename, corpus_results_path, corpus_results_filename\n",
        "    \"\"\"paths\"\"\"\n",
        "    # mount google drive if colab boolean True\n",
        "    mount_google_drive() if colab else None\n",
        "    # path on google drive to a data directory\n",
        "    google_drive_path = \"/content/drive/MyDrive/\"\n",
        "    # path to annotations\n",
        "    annotations_path = f'{google_drive_path if colab else \"./\"}data/{dataset}/'\n",
        "    # path to the model\n",
        "    trained_model_path = f'{google_drive_path if colab else \"./\"}data/model/'\n",
        "    # path for tested documents results\n",
        "    doc_results_path = f'{google_drive_path if colab else \"./\"}data/results/'\n",
        "    # filename for tested documents results\n",
        "    doc_results_filename = f\"doc_{model}.csv\"\n",
        "    # path for corpus results\n",
        "    corpus_results_path = f'{google_drive_path if colab else \"./\"}data/results/'\n",
        "    # filename for corpus results\n",
        "    corpus_results_filename = f\"corpus_{model}.csv\"\n",
        "    annotations_data_filetype = \"json\"\n",
        "\n",
        "    \"\"\"globals set here\"\"\"\n",
        "    global annotated_files, labels_of_interest\n",
        "    # define entity labels of interest\n",
        "    labels_of_interest = [\"GPE\", \"LOC\", \"DATE\", \"TIME\", \"COLOR\", \"TYPE\"]\n",
        "    # run setup functions\n",
        "    get_annotation_urls(annotations_path, annotations_data_filetype, print_output)\n",
        "    json_to_doc(print_output)\n",
        "    load_models(print_output, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgnX6E4xUSv7"
      },
      "outputs": [],
      "source": [
        "# function to get all entities in a hand-annotated doc (all paras)\n",
        "def get_annotated_entities(annotations):\n",
        "    return [para[1][\"entities\"] for para in annotations]\n",
        "\n",
        "\n",
        "# function to get all raw text from the hand-annotated doc (all paras)\n",
        "def get_text(annotations):\n",
        "    return [para[0] for para in annotations]\n",
        "\n",
        "\n",
        "# function to get the annotations\n",
        "def get_annotations(print_output=False):\n",
        "    global annotated_entities, annotated_text\n",
        "\n",
        "    \"\"\"\n",
        "    Note: Entities to be stored in the form [[[element1, element2]],[[element1, element2]]]\n",
        "    \"\"\"\n",
        "    # run function to get all entities from all paragraphs in all passed-in hand-annotated docs\n",
        "    annotated_entities = [\n",
        "        get_annotated_entities(doc[\"annotations\"]) for doc in annotations\n",
        "    ]\n",
        "    # run function to get all text from all paragraphs in all passed-in hand-annotated docs\n",
        "    annotated_text = [get_text(doc[\"annotations\"]) for doc in annotations]\n",
        "\n",
        "    \"\"\"\n",
        "    Note: Annotated text stored in form [[para1, para1],[para1, para2]]\n",
        "    i.e., a list of document-lists of paras\n",
        "    \"\"\"\n",
        "    if print_output:\n",
        "        # print total counts of annotated documents; paragraphs & entities\n",
        "        print(f\"Number of documents: {len(annotated_entities)}\")\n",
        "        print(f\"Number of paras: {sum([len(x) for x in annotated_entities])}\")\n",
        "        print(\n",
        "            f\"Number of entities: {sum([sum(len(y) for y in x ) for x in annotated_entities])}\\n\"\n",
        "        )\n",
        "        # print first entity, of first paragraph, of first doc, to verify entities\n",
        "        print(\n",
        "            f\"Annotated entities sample (doc 4, para 1): {annotated_entities[3][0]}\\n\"\n",
        "        )\n",
        "        # print sample of annotated text to verify\n",
        "        print(f\"Annotated text sample (doc 4, para 1): {annotated_text[3][0]}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjO5Bpw_USv7"
      },
      "outputs": [],
      "source": [
        "# function to run the NER\n",
        "def run_ner(model, print_output=False):\n",
        "    global extracted_entities\n",
        "    # returned in the form [[[[element1, element2]],[doc2[element1, element2]]]]\n",
        "    extracted_entities = []\n",
        "    for doc_text in annotated_text:\n",
        "        doc_ents = []\n",
        "        for para in nlp.pipe(\n",
        "            doc_text, disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]\n",
        "        ):\n",
        "            doc_ents.append(\n",
        "                [[ent.start_char, ent.end_char, ent.label_] for ent in para.ents]\n",
        "            )\n",
        "        extracted_entities.append(doc_ents)\n",
        "\n",
        "    if print_output:\n",
        "        # print total counts of processed documents; paragraphs & entities\n",
        "        print(f\"Number of documents: {len(extracted_entities)}\")\n",
        "        print(f\"Number of paras: {sum([len(x) for x in extracted_entities])}\")\n",
        "        print(\n",
        "            f\"Number of entities: {sum([sum(len(y) for y in x ) for x in extracted_entities])}\"\n",
        "        )\n",
        "        print(\"\\n\")\n",
        "        print(f\"Extract entities: {extracted_entities}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4UTT0-XUSv8"
      },
      "outputs": [],
      "source": [
        "# function to count annotated and extracted entities in doc\n",
        "def count_entities(doc):\n",
        "    return sum([len(list(ent for ent in para)) for para in doc]) if any(doc) else 0\n",
        "\n",
        "\n",
        "# function to count number of sample paragraphs\n",
        "def count_sample_paragraphs():\n",
        "    return sum([len(doc[\"annotations\"]) for doc in annotations])\n",
        "\n",
        "\n",
        "# function to count the entity class label types\n",
        "def count_entity_types(dataset=\"annotations\"):\n",
        "    global entity_count\n",
        "    entity_count = {k: 0 for k in labels_of_interest}\n",
        "    for doc in annotated_entities if dataset == \"annotations\" else extracted_entities:\n",
        "        for para in doc:\n",
        "            for ent in para:\n",
        "                entity_count[ent[2]] += 1\n",
        "    return entity_count\n",
        "\n",
        "\n",
        "# function to run the counts\n",
        "def run_counts(print_output=False):\n",
        "    global doc_extracted_entities_count, doc_annotated_entities_count, corpus_extracted_entities_count, corpus_annotated_entities_count, extracted_entities, annotated_entities, corpus_sample_paras_total\n",
        "    # count all entities for each doc\n",
        "    doc_extracted_entities_count = [count_entities(doc) for doc in extracted_entities]\n",
        "    doc_annotated_entities_count = [count_entities(doc) for doc in annotated_entities]\n",
        "    # count all sample paragraphs for corpus\n",
        "    corpus_sample_paras_total = count_sample_paragraphs()\n",
        "    # count all entities for corpus\n",
        "    corpus_extracted_entities_count = sum(doc_extracted_entities_count)\n",
        "    corpus_annotated_entities_count = sum(doc_annotated_entities_count)\n",
        "    # print output (always)\n",
        "    print(\n",
        "        f\"\\nExtracted entities count for each doc: {doc_extracted_entities_count}\\nAnnotated entities count for each doc: {doc_annotated_entities_count}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"\\nTotal annotated sample paragraphs in the corpus: {corpus_sample_paras_total}\\n\"\n",
        "    )\n",
        "    print(f\"Extracted entities count for corpus: {corpus_extracted_entities_count}\")\n",
        "    print(f\"Annotated entities count for corpus: {corpus_annotated_entities_count}\\n\")\n",
        "    print(f'Totals of annotated entity types: {count_entity_types(\"annotations\")}')\n",
        "    print(f'Totals of extracted entity types: {count_entity_types(\"ner\")}')\n",
        "    # print output (optional)\n",
        "    if print_output:\n",
        "        print(f\"\\nExtracted entities for corpus: {extracted_entities}\")\n",
        "        print(f\"Annotated entities for corpus: {annotated_entities}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxOB7DthUSv8"
      },
      "outputs": [],
      "source": [
        "# function to remove any entities not of interest in doc\n",
        "def remove_irrelevant_entities(doc):\n",
        "    \"\"\"\n",
        "    Note: This ensures extra detected entities do not influence performance\n",
        "    calculations and is useful for calculating performance\n",
        "    without the influence of custom entities\n",
        "    \"\"\"\n",
        "    for idx, para in enumerate(doc):\n",
        "        doc[idx] = [ent for ent in para if ent[2] in labels_of_interest]\n",
        "    return doc\n",
        "\n",
        "\n",
        "# function to perform preprocessing on the data\n",
        "def preprocess_data(print_output=False):\n",
        "    global extracted_entities, annotated_entities, annotated_text, test_data\n",
        "    # extract irrelevant entities\n",
        "    extracted_entities = [remove_irrelevant_entities(doc) for doc in extracted_entities]\n",
        "    annotated_entities = [remove_irrelevant_entities(doc) for doc in annotated_entities]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBrl8eeqUSv8"
      },
      "outputs": [],
      "source": [
        "# function to run the analysis process\n",
        "def analyse(print_output=False):\n",
        "\n",
        "    # define globals\n",
        "    global doc_extracted_entities_count, doc_annotated_entities_count, corpus_extracted_entities_count, corpus_annotated_entities_count, extracted_entities, corpus_false_positive_entities, true_positives, false_negatives, corpus_true_positives_total, doc_results\n",
        "\n",
        "    # function to find true positives and false negatives, per document\n",
        "    def find_matches(print_output, doc_idx, annotated, extracted):\n",
        "        true_pos = 0\n",
        "        false_neg = (\n",
        "            doc_annotated_entities_count[doc_idx]\n",
        "            - doc_extracted_entities_count[doc_idx]\n",
        "        )\n",
        "        for para_idx, (a_ents, e_ents) in enumerate(zip(annotated, extracted)):\n",
        "            ee_matched = []\n",
        "            ae_matched = []\n",
        "            # look for extracted entities within annotated entity boundaries (matches)\n",
        "            for ae in a_ents:\n",
        "                for ee in e_ents:\n",
        "                    if ee[1] >= ae[0] and ee[0] <= ae[1]:\n",
        "                        if ee[2] == ae[2]:  # true positive identified!\n",
        "                            if ee in ee_matched:\n",
        "                                false_neg -= 1  # decrement false negatives by 1\n",
        "                            else:\n",
        "                                ee_matched.append(\n",
        "                                    ee\n",
        "                                )  # record NER identified entity as seen\n",
        "                                true_pos += 1  # increment true positives by 1\n",
        "                            if ae in ae_matched:\n",
        "                                false_neg += 1  # increment false negatives by 1\n",
        "                            else:\n",
        "                                ae_matched.append(ae)  # record appended entity as seen\n",
        "        \"\"\"\n",
        "        Note: Following is to prevent false_neg falling < 0 in event that \n",
        "        more NER extracted entities than annotated unduly influence this calculation\n",
        "        \"\"\"\n",
        "        false_neg = false_neg if false_neg > 0 else 0\n",
        "        return true_pos, false_neg\n",
        "\n",
        "    # function to calculate true positives for the corpus\n",
        "    def calc_corpus_true_positives_total(print_output):\n",
        "        global corpus_true_positives_total\n",
        "        corpus_true_positives_total = sum(true_positives)\n",
        "\n",
        "    # function to calculate false negatives for the corpus\n",
        "    def calc_corpus_false_negatives_total(print_output):\n",
        "        global corpus_false_negatives_total\n",
        "        corpus_false_negatives_total = sum(false_negatives)\n",
        "\n",
        "    # function to calculate true positive & false negatives for each doc\n",
        "    def calc_true_pos_false_neg(print_output):\n",
        "        global true_positives, false_negatives\n",
        "        true_positives = []\n",
        "        false_negatives = []\n",
        "        for doc_idx, doc in enumerate(annotated_entities):\n",
        "            true_pos, false_neg = find_matches(\n",
        "                print_output,\n",
        "                doc_idx,\n",
        "                annotated_entities[doc_idx],\n",
        "                extracted_entities[doc_idx],\n",
        "            )\n",
        "            true_positives.append(true_pos)\n",
        "            false_negatives.append(false_neg)\n",
        "\n",
        "    # functions to compute precision, recall and f1-score for model-level evaluation\n",
        "    def model_level_eval_doc():\n",
        "        global doc_results\n",
        "        doc_results = []\n",
        "        precision_list = []\n",
        "        recall_list = []\n",
        "        f1_score_list = []\n",
        "\n",
        "        for tp, fn, ee in zip(\n",
        "            true_positives, false_negatives, doc_extracted_entities_count\n",
        "        ):\n",
        "            # calculate precision for each doc\n",
        "            precision_list.append(tp / ee) if tp > 0 else precision_list.append(\n",
        "                1.0\n",
        "            ) if (fn == 0 and ee == 0) else precision_list.append(0)\n",
        "            # calculate recall for each doc\n",
        "            recall_list.append(tp / (tp + fn)) if tp > 0 else recall_list.append(\n",
        "                1.0\n",
        "            ) if (fn == 0 and ee == 0) else recall_list.append(0)\n",
        "        for idx, (p, r, tp, fn, ee) in enumerate(\n",
        "            zip(\n",
        "                precision_list,\n",
        "                recall_list,\n",
        "                true_positives,\n",
        "                false_negatives,\n",
        "                doc_extracted_entities_count,\n",
        "            )\n",
        "        ):\n",
        "            # calculate f1-score for each doc\n",
        "            f1_score_list.append((2 * p * r / (p + r))) if (\n",
        "                p > 0 and r > 0\n",
        "            ) else f1_score_list.append(1.0) if (\n",
        "                tp == 0 and (ee == 0 and fn == 0)\n",
        "            ) else f1_score_list.append(\n",
        "                0\n",
        "            )\n",
        "        # add results to results dictionary\n",
        "        for doc in range(len(precision_list)):\n",
        "            doc_results.append(\n",
        "                {\n",
        "                    \"precision\": round(precision_list[doc], 3),\n",
        "                    \"recall\": round(recall_list[doc], 3),\n",
        "                    \"f1-score\": round(f1_score_list[doc], 3),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # function to calculate precision of corpus\n",
        "    def model_level_eval_corpus():\n",
        "        global corpus_results\n",
        "        precision = (\n",
        "            corpus_true_positives_total / corpus_extracted_entities_count\n",
        "            if corpus_true_positives_total > 0\n",
        "            else 1.0\n",
        "            if (\n",
        "                corpus_extracted_entities_count == 0\n",
        "                and corpus_false_negatives_total == 0\n",
        "            )\n",
        "            else 0\n",
        "        )\n",
        "        # calculate recall for each doc\n",
        "        recall = (\n",
        "            corpus_true_positives_total\n",
        "            / (corpus_true_positives_total + corpus_false_negatives_total)\n",
        "            if corpus_true_positives_total > 0\n",
        "            else 1.0\n",
        "            if (\n",
        "                corpus_extracted_entities_count == 0\n",
        "                and corpus_false_negatives_total == 0\n",
        "            )\n",
        "            else 0\n",
        "        )\n",
        "        # calculate f1-score for each doc\n",
        "        f1_score = (\n",
        "            2 * precision * recall / (precision + recall)\n",
        "            if (precision > 0 and recall > 0)\n",
        "            else 1.0\n",
        "            if corpus_true_positives_total == 0\n",
        "            and (\n",
        "                corpus_extracted_entities_count == 0\n",
        "                and corpus_false_negatives_total == 0\n",
        "            )\n",
        "            else 0\n",
        "        )\n",
        "        corpus_results = {\n",
        "            \"precision\": round(precision, 3),\n",
        "            \"recall\": round(recall, 3),\n",
        "            \"f1_score\": round(f1_score, 3),\n",
        "        }\n",
        "\n",
        "    # define variables to hold the working data\n",
        "    corpus_false_positive_entities = []\n",
        "    corpus_missed_entities = []\n",
        "    corpus_false_positive_entities.clear()  # clear list\n",
        "    corpus_missed_entities.clear()  # clear list first\n",
        "\n",
        "    # run the above functions to evaluate\n",
        "    preprocess_data(print_output=0)  # create working copies of data\n",
        "    run_counts(0)\n",
        "    calc_true_pos_false_neg(print_output)\n",
        "    calc_corpus_true_positives_total(print_output)\n",
        "    calc_corpus_false_negatives_total(print_output)\n",
        "    model_level_eval_doc()\n",
        "    model_level_eval_corpus()\n",
        "\n",
        "    # print the output\n",
        "    if print_output:\n",
        "        print(\"\\n\")\n",
        "        # print per document totals\n",
        "        for doc_idx, doc in enumerate(extracted_entities):\n",
        "            print(f\"Document {doc_idx}\")\n",
        "            print(f\"True positives: {true_positives[doc_idx]}\")\n",
        "            print(\n",
        "                f\"False positives: {doc_extracted_entities_count[doc_idx] - true_positives[doc_idx]}\"\n",
        "            )\n",
        "            print(f\"False negatives: {false_negatives[doc_idx]}\\n\")\n",
        "        # print all totals\n",
        "        print(f\"\\nDocument analysis results: {doc_results}\")\n",
        "        print(f\"Corpus analysis results {corpus_results}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbOueK6gUSv9"
      },
      "outputs": [],
      "source": [
        "# function to display examples using the displacy module\n",
        "def display_examples(docs_of_interest):\n",
        "    for doc in docs_of_interest:\n",
        "        try:\n",
        "            for para_idx, para in enumerate(annotated_entities[doc]):\n",
        "                ae = annotated_entities[doc][para_idx]\n",
        "                text = annotated_text[doc][para_idx]\n",
        "                print(f'\\nAnnotated example for document {doc}')\n",
        "                displacy.render({\n",
        "                'text': text,\n",
        "                'ents': [{\"start\": e[0], \"end\": e[1], \"label\": e[2]} for e in ae],\n",
        "                \"title\": f'Document {doc}, para {para_idx}'\n",
        "            }, manual=True, style='ent', jupyter=True)\n",
        "                print(f'Extracted example for document {doc}')\n",
        "                ee = extracted_entities[doc][para_idx]\n",
        "                displacy.render({\n",
        "                'text': text,\n",
        "                'ents': [{\"start\": e[0], \"end\": e[1], \"label\": e[2]} for e in ee],\n",
        "                \"title\": f'Document {doc}, para {para_idx}'\n",
        "            }, manual=True, style='ent', jupyter=True)\n",
        "        except IndexError:\n",
        "            print(f'You appear to be trying to display results for document {doc}, which does not appear to exist!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFGW4K0kUSv-"
      },
      "outputs": [],
      "source": [
        "# function to display results using a pandas dataframe\n",
        "def display_results(show_label_examples=[], show_scores=True):\n",
        "    global df_docs_results, df_corpus_results\n",
        "    if show_label_examples:\n",
        "        display_examples(show_label_examples)\n",
        "    if show_scores:\n",
        "        #Â convert Python dictionary to pandas dataframe\n",
        "        dr = dict()\n",
        "        for idx, doc in enumerate(doc_results):\n",
        "            dr[idx] = {k.capitalize(): v for k,v in doc.items()}\n",
        "        print('\\nDocument Analysis Results')\n",
        "        df_docs_results = pd.DataFrame.from_dict(dr)\n",
        "        df_docs_results = df_docs_results.T\n",
        "        df_docs_results = df_docs_results[['Precision','Recall','F1-score']]\n",
        "        df_docs_results.index.name = 'Document'\n",
        "        display(df_docs_results)\n",
        "        print('\\nCorpus Analysis Results')\n",
        "        cr = {k.capitalize():[v] for k, v in corpus_results.items()}\n",
        "        df_corpus_results = pd.DataFrame.from_dict(cr)\n",
        "        df_corpus_results.index.name = 'Corpus'\n",
        "        display(df_corpus_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWka_K6eUSv-"
      },
      "outputs": [],
      "source": [
        "# function to write results to a file\n",
        "def write_results_to_file(write):\n",
        "    if write:\n",
        "        df_docs_results.to_csv(doc_results_path + doc_results_filename)\n",
        "        df_corpus_results.to_csv(corpus_results_path + corpus_results_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bz7V9wMUSv-",
        "outputId": "c5c92799-fc00-4725-fc69-69388e9848b6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Configure the testing.\n",
        "\n",
        "Arguments:\n",
        "\n",
        "model: The model to test. Options include 'en_core_web_lg', 'en_core_web_trf', 'stanza', and 'trf-model-best'\n",
        "dataset: The dataset to use, from: 'train' or 'test'.\n",
        "print_output: Whether to print verbose output (boolean)\n",
        "colab: Whether this is run on the Google Colaboratory platform (boolean)\n",
        "\n",
        "\"\"\"\n",
        "global model\n",
        "model = \"cnn-model-best\"\n",
        "setup(model=model, dataset=\"test\", print_output=0, colab=1)\n",
        "# import the annotations\n",
        "get_annotations(print_output=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1h6oP07Xal8"
      },
      "outputs": [],
      "source": [
        "# run the NER process, using the configuration defined above\n",
        "run_ner(model, print_output=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q3NWbPf7Xa6g",
        "outputId": "7f8bd2e9-e9c5-4644-9309-05a820f4250a"
      },
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "Run the analysis process, using the configuration defined above.\n",
        "\n",
        "Arguments: \n",
        "\n",
        "analyse: Whether to print the output (boolean)\n",
        "display_results: ([Indexes (counting from 1) of docs to display entities for (list)], show scores? (boolean))\n",
        "write_results_to_file: Whether to write the results of the testing to an output file (boolean)\n",
        "\n",
        "\"\"\"\n",
        "analyse(print_output=0)\n",
        "display_results([4], 1)\n",
        "write_results_to_file(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4MQN_sQU5BP",
        "outputId": "d3220eca-83fe-4d8e-8f7d-4c045cad13ce"
      },
      "outputs": [],
      "source": [
        "# function to print entities extracted for semantic testing\n",
        "def print_ents(doc):\n",
        "    if doc.ents:\n",
        "        for ent in doc.ents:\n",
        "            print(f\"Text: {ent.text}\")\n",
        "            print(f\"Label: {ent.label_}\")\n",
        "    else:\n",
        "        print(\"None\")\n",
        "\n",
        "\n",
        "# define the data for the semantic test\n",
        "token = \"disk\"\n",
        "token_in_context = f\"I saw an object in the sky, it looked like a {token}\"\n",
        "token_in_wrong_context = f\"I played a {token} on my CD player!\"\n",
        "\n",
        "# run the semantic test\n",
        "print(f\"Token absent context: \")\n",
        "print_ents(nlp(token))\n",
        "print(f\"\\nToken in correct context: \")\n",
        "print_ents(nlp(token_in_context))\n",
        "print(f\"\\nToken in incorrect context: \")\n",
        "print_ents(nlp(token_in_wrong_context))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.14 ('uhi-ZQVV2iWc')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "65f88eacf3a10b22e2367da6754b23494b2804a02fe03c23afbe72788a968f5d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
