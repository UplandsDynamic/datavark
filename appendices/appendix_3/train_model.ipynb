{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NER model training notebook\n",
        "Author name: Daniel J. S. Bright\n",
        "Author contact: 12004727@uhi.ac.uk\n",
        "Date last touched: 23 February 2023\n",
        "Description: Jupyter Notebook sheet to train spaCy NER systems. Trains the spaCy CNN and transformer-based models, using either a CPU or GPU. Created for a dissertation for the MSc in Web Technologies at University of Highlands & Islands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rWgjJdhqOZn",
        "outputId": "a2d4a004-742b-4a50-d558-e6a3ea541319"
      },
      "outputs": [],
      "source": [
        "# install spaCy transformers\n",
        "!pip install spacy spacy-transformers\n",
        "# install base spaCy CNN model\n",
        "!python -m spacy download en_core_web_lg\n",
        "# install base spaCy transformer model\n",
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DaeTdUDnS1n"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import spacy, json, glob, os, random\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.util import filter_spans\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUc--y07nS1r"
      },
      "outputs": [],
      "source": [
        "# function to get references to the annotated files\n",
        "def get_annotation_file_handles(train_url, ext, print_output=True):\n",
        "    global annotated_files\n",
        "\n",
        "    def construct_handles(url):\n",
        "        # ensure url has trailing slash\n",
        "        url = url + \"/\" if url[-1:] != \"/\" else url\n",
        "        # load hand annotated examples\n",
        "        annotated_files = glob.glob(url + f\"*.{ext}\")\n",
        "        # sort based on filename\n",
        "        annotated_files.sort(key=lambda x: os.path.basename(x))\n",
        "        # print counted files to demonstrate success\n",
        "        if print_output:\n",
        "            print(f\"Number of annotated files: {len(annotated_files)}\")\n",
        "        return annotated_files\n",
        "\n",
        "    # construct handles for training data\n",
        "    annotated_files = construct_handles(train_url)\n",
        "\n",
        "\n",
        "# function to load json into list of Python dictionaries\n",
        "def json_to_doc(print_output=False):\n",
        "    global annotations\n",
        "\n",
        "    def create_annotation_list(json_files):\n",
        "        annotation_list = []\n",
        "        for f in json_files:\n",
        "            with open(f, \"r\", encoding=\"utf-8\") as file:\n",
        "                annotation_list.append(json.loads(file.read()))\n",
        "        if print_output:\n",
        "            # print count of annotation dicts to verify success\n",
        "            print(f\"Number of annotations in files: {len(annotation_list)}\")\n",
        "            # print first element (document), to verify\n",
        "            print(f\"Annotation sample: {annotation_list[:1]}\")\n",
        "        return annotation_list\n",
        "\n",
        "    annotations = create_annotation_list(annotated_files)\n",
        "\n",
        "\n",
        "# function to split the data into test and train sets\n",
        "def test_train_split(print_output=False):\n",
        "    global annotations_training, annotations_dev\n",
        "    random.shuffle(annotations)\n",
        "    annotations_training = annotations[0 : int(len(annotations) * 0.8)]\n",
        "    annotations_dev = annotations[len(annotations_training) :]\n",
        "    print(\n",
        "        f\"\\nTraining data is {len(annotations_training)} documents, dev data is {len(annotations_dev)} documents.\\n\"\n",
        "    ) if print_output else None\n",
        "    print(\n",
        "        f\"\\nFirst training document (to test randomisation): {annotations_training[0]}\\n\"\n",
        "    ) if print_output else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDo6ld3inS1s"
      },
      "outputs": [],
      "source": [
        "# function to set up the training\n",
        "def setup(print_output=0, colab=0):\n",
        "    # set global variables\n",
        "    global annotated_files, labels_of_interest, docbin_object_path, docbin_object_training_filename, docbin_object_dev_filename\n",
        "    # set local variables - define paths, filenames, etc.\n",
        "    google_drive_path = \"/content/drive/MyDrive/\"\n",
        "    annotations_data_path = (\n",
        "        f'{google_drive_path if colab else \"./\"}data/training_annotations'\n",
        "    )\n",
        "    annotations_data_filetype = \"json\"\n",
        "    docbin_object_path = f'{google_drive_path if colab else \"./\"}docbin/'  # important: remember trailing slash\n",
        "    docbin_object_training_filename = \"training_data.spacy\"\n",
        "    docbin_object_dev_filename = \"dev_data.spacy\"\n",
        "    # define the entity class labels to train NER for (include both pre-trained & custom)\n",
        "    labels_of_interest = [\"GPE\", \"LOC\", \"DATE\", \"TIME\", \"COLOR\", \"TYPE\"]\n",
        "    # run the previously defined setup functions\n",
        "    get_annotation_file_handles(\n",
        "        annotations_data_path, annotations_data_filetype, print_output\n",
        "    )\n",
        "    json_to_doc(print_output)\n",
        "    test_train_split(print_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVdltRn4nS1t"
      },
      "outputs": [],
      "source": [
        "# function to count the number of samples (paragraphs) in the training corpus\n",
        "def count_training_samples():\n",
        "    return sum([len(doc[\"annotations\"]) for doc in annotations_training])\n",
        "\n",
        "\n",
        "# function to count the number of samples (paragraphs) for the training corpus\n",
        "def count_dev_samples():\n",
        "    return sum([len(doc[\"annotations\"]) for doc in annotations_dev])\n",
        "\n",
        "\n",
        "# function to get all entities in a hand-annotated doc (all lines)\n",
        "def get_annotated_entities(annotations):\n",
        "    return [line[1][\"entities\"] for line in annotations]\n",
        "\n",
        "\n",
        "# function to get all raw text from the hand-annotated doc (all lines)\n",
        "def get_text(annotations):\n",
        "    return [line[0] for line in annotations]\n",
        "\n",
        "\n",
        "# function to load the annotated data\n",
        "def get_annotations(print_output=False):\n",
        "    \"\"\"\n",
        "    Note:\n",
        "    Entities to be stored in the form [[[element1, element2]],[[element1, element2]]].\n",
        "    Annotated text stored in form [[line1, line1],[line1, line2]].\n",
        "    \"\"\"\n",
        "\n",
        "    # define globals\n",
        "    global annotated_entities_training, annotated_entities_dev, annotated_text_training, annotated_text_dev\n",
        "    # run functions to get all entities, from all lines, in all the passed-in hand-annotated docs\n",
        "    annotated_entities_training = [\n",
        "        get_annotated_entities(doc[\"annotations\"]) for doc in annotations_training\n",
        "    ]\n",
        "    annotated_entities_dev = [\n",
        "        get_annotated_entities(doc[\"annotations\"]) for doc in annotations_dev\n",
        "    ]\n",
        "    # run functions to get all text, from all lines, in all the passed-in hand-annotated docs\n",
        "    annotated_text_training = [\n",
        "        get_text(doc[\"annotations\"]) for doc in annotations_training\n",
        "    ]\n",
        "    annotated_text_dev = [get_text(doc[\"annotations\"]) for doc in annotations_dev]\n",
        "    # print total counts of annotated documents; lines and entities\n",
        "    if print_output:\n",
        "        print(f\"Number of training documents: {len(annotated_entities_training)}\")\n",
        "        print(\n",
        "            f\"Number of training samples (paragraphs) in training documents: {count_training_samples()}\"\n",
        "        )\n",
        "        print(f\"Number of dev documents: {len(annotated_entities_dev)}\")\n",
        "        print(\n",
        "            f\"Number of dev samples (paragraphs) in dev documents: {count_dev_samples()}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Number of training lines: {sum([len(x) for x in annotated_entities_training])}\"\n",
        "        )\n",
        "        print(f\"Number of dev lines: {sum([len(x) for x in annotated_entities_dev])}\")\n",
        "        print(\n",
        "            f\"Number of training entities: {sum([sum(len(y) for y in x ) for x in annotated_entities_training])}\\n\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Number of dev entities: {sum([sum(len(y) for y in x ) for x in annotated_entities_dev])}\\n\"\n",
        "        )\n",
        "        # print first entity, of first line, of first doc, to verify entities\n",
        "        print(\n",
        "            f\"Annotated entities training sample (doc 4, line 1): {annotated_entities_training[3][0]}\\n\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Annotated entities dev sample (doc 4, line 1): {annotated_entities_dev[3][0]}\\n\"\n",
        "        )\n",
        "        # print sample of annotated text to verify\n",
        "        print(\n",
        "            f\"Annotated text training sample (doc 2, line 1): {annotated_text_training[1][0]}\\n\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Annotated text dev sample (doc 2, line 1): {annotated_text_dev[1][0]}\\n\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kABeOc0nS1v"
      },
      "outputs": [],
      "source": [
        "# function to compile the training data\n",
        "def compile_training_data(print_output):\n",
        "    global training_data, dev_data\n",
        "    training_data = dict()\n",
        "    dev_data = dict()\n",
        "    training_annotations = list()\n",
        "    dev_annotations = list()\n",
        "\n",
        "    for doc_idx, doc in enumerate(annotated_entities_training):\n",
        "        for line_idx, line in enumerate(doc):\n",
        "            ents = list()\n",
        "            for ent in line:\n",
        "                ents.append((ent[0], ent[1], ent[2]))\n",
        "            training_annotations.append(\n",
        "                {\"entities\": ents, \"text\": annotated_text_training[doc_idx][line_idx]}\n",
        "            )\n",
        "    training_data[\"classes\"] = labels_of_interest\n",
        "    training_data[\"annotations\"] = training_annotations\n",
        "\n",
        "    for doc_idx, doc in enumerate(annotated_entities_dev):\n",
        "        for line_idx, line in enumerate(doc):\n",
        "            ents = list()\n",
        "            for ent in line:\n",
        "                ents.append((ent[0], ent[1], ent[2]))\n",
        "            dev_annotations.append(\n",
        "                {\"entities\": ents, \"text\": annotated_text_dev[doc_idx][line_idx]}\n",
        "            )\n",
        "    dev_data[\"classes\"] = labels_of_interest\n",
        "    dev_data[\"annotations\"] = dev_annotations\n",
        "\n",
        "    # print sample of compiled annotation training data\n",
        "    print(\n",
        "        f'Training data sample (doc 1, line 1): {training_data.get(\"annotations\")[2]}\\n'\n",
        "    ) if print_output else None\n",
        "    # print sample of compiled annotation dev data\n",
        "    print(\n",
        "        f'Training data sample (doc 1, line 1): {dev_data.get(\"annotations\")[2]}\\n'\n",
        "    ) if print_output else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zN2ZW4wnS1v"
      },
      "outputs": [],
      "source": [
        "# function to prepare training & create spaCy binary config file\n",
        "def prepare_training(print_output):\n",
        "\n",
        "    nlp = spacy.blank(\"en\")  # load a new spacy model\n",
        "\n",
        "    # function to create the spaCy binary training config file\n",
        "    def create_spacy_file(path, filename, data):\n",
        "        doc_bin = DocBin()\n",
        "        skipped_ent_count = 0\n",
        "        filtered_ents_count = 0\n",
        "        for idx, training_line in tqdm(enumerate(data)):\n",
        "            text = training_line[\"text\"]\n",
        "            labels = training_line[\"entities\"]\n",
        "            doc = nlp.make_doc(text)\n",
        "            ents = []\n",
        "            skipped_ents = 0\n",
        "            for start, end, label in labels:\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "                if span is None:\n",
        "                    skipped_ents += 1\n",
        "                else:\n",
        "                    ents.append(span)\n",
        "            filtered_ents = filter_spans(ents)\n",
        "            skipped_ent_count += skipped_ents\n",
        "            filtered_ents_count += len(filtered_ents)\n",
        "            doc.ents = filtered_ents\n",
        "            doc_bin.add(doc)\n",
        "        print(\n",
        "            f\"Number of skipped entities: {skipped_ent_count}\"\n",
        "        ) if print_output else None\n",
        "        print(\n",
        "            f\"Number of filtered entities: {filtered_ents_count}\"\n",
        "        ) if print_output else None\n",
        "        doc_bin.to_disk(path + filename)\n",
        "\n",
        "    # create training data\n",
        "    create_spacy_file(\n",
        "        docbin_object_path,\n",
        "        docbin_object_training_filename,\n",
        "        data=training_data[\"annotations\"],\n",
        "    )\n",
        "    # create dev data\n",
        "    create_spacy_file(\n",
        "        docbin_object_path, docbin_object_dev_filename, data=dev_data[\"annotations\"]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zu_wk-6kJsii"
      },
      "outputs": [],
      "source": [
        "# function to mount the Google Drive\n",
        "def mount_google_drive(colab=False):\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph64Wvf3nS1w",
        "outputId": "64b5d1f0-bad6-468b-8a6e-e0203e07d8b4"
      },
      "outputs": [],
      "source": [
        "# run previously defined functions to initiate the training\n",
        "\"\"\" \n",
        "Arguments & variables:\n",
        "colab: Whether running on Google Colaboratory (boolean)\n",
        "setup: Whether to print output (boolean), pass in the colab variable\n",
        "get_annotations: Whether to print output of this step (boolean)\n",
        "compile_training_data: Whether to print output of this step (boolean)\n",
        "prepare_training: Whether to print output of this step (boolean)\n",
        "\"\"\"\n",
        "\n",
        "colab = 1\n",
        "mount_google_drive() if colab else None\n",
        "setup(0, colab)\n",
        "get_annotations(1)\n",
        "compile_training_data(1)\n",
        "prepare_training(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-VWNd6CnS1x"
      },
      "outputs": [],
      "source": [
        "# Run the model training, using a CPU\n",
        "\n",
        "# add defaults to base config created at https://spacy.io/usage/training#quickstart\n",
        "#!python -m spacy init fill-config /content/drive/MyDrive/data/base_config_cnn_accuracy.cfg /content/drive/MyDrive/data/config.cfg\n",
        "\n",
        "# train the model (CPU)\n",
        "#!python -m spacy train /content/drive/MyDrive/data/config.cfg --output /content/drive/MyDrive/data/ --paths.train /content/drive/MyDrive/data/docbin/training_data.spacy --paths.dev /content/drive/MyDrive/data/docbin/dev_data.spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzJQCkTR0WoM",
        "outputId": "4cbe16b0-86c4-42ec-9ce6-695de7ab1ddb"
      },
      "outputs": [],
      "source": [
        "# Run the model training, using a GPU\n",
        "#Â add defaults to base config created at https://spacy.io/usage/training#quickstart\n",
        "!python -m spacy init fill-config /content/drive/MyDrive/data/base_config_transformer_accuracy.cfg /content/drive/MyDrive/data/config.cfg\n",
        "\n",
        "# train the model (GPU)\n",
        "!python -m spacy train /content/drive/MyDrive/data/config.cfg --output /content/drive/MyDrive/data/ --paths.train /content/drive/MyDrive/data/docbin/training_data.spacy --paths.dev /content/drive/MyDrive/data/docbin/training_data.spacy --gpu-id 0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.15 ('uhi-ZQVV2iWc')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "65f88eacf3a10b22e2367da6754b23494b2804a02fe03c23afbe72788a968f5d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
